{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis for Information Retrieval - Part 1\n",
    "\n",
    "Elyes KHALFALLAH & Mohammed ali EL ADLOUNI\n",
    "\n",
    "16/03/2025\n",
    "\n",
    "---\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Etapes préliminaires\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful libraries\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fonctions import *\n",
    "\n",
    "# Downloading the necessary datasets for the nltk library\n",
    "# Only download if necessary\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from data_project.csv\n",
    "data = pd.read_csv(\"data_project.csv\", sep=\"\\t\")\n",
    "\n",
    "# Show the first 5 rows of the data\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"class\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Prétraitement des données\n",
    "\n",
    "### 1.1. Générer le texte sur lequel nous travaillerons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in 'abstract' and 'title' with empty strings and create 'text'\n",
    "data_text = data.fillna({\"abstract\": \"\", \"title\": \"\"})\n",
    "\n",
    "# Concatenate 'title' and 'abstract' into 'text'\n",
    "data_text[\"text\"] = \" \" + data_text[\"title\"] + \" \" + data_text[\"abstract\"]\n",
    "\n",
    "# Reposition 'text' as the first column\n",
    "cols = [\"text\"] + [col for col in data_text.columns if col != \"text\"]\n",
    "data_text = data_text[cols]\n",
    "\n",
    "# Show the first 5 rows of the cleaned data\n",
    "data_text.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 'text' column of the first row\n",
    "print(data_text[\"text\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Prétrairements poussés\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to build an index for the data. To do so, we'll :\n",
    "\n",
    "1. Remove punctuation\n",
    "2. Lowercase everything\n",
    "3. Remove useless spaces\n",
    "4. Remove stop-words\n",
    "5. Normalize the data (lemmatization)\n",
    "6. Remove outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punctuation removal :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all puntuation from 'text' using regex\n",
    "data_text[\"text\"] = data_text[\"text\"].apply(lambda x: re.sub(r\"[^\\w\\s]\", \" \", x))\n",
    "\n",
    "# Show 'text' column of the first row\n",
    "print(data_text[\"text\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'text' to lowercase\n",
    "data_text[\"text\"] = data_text[\"text\"].str.lower()\n",
    "\n",
    "# Show 'text' column of the first row\n",
    "print(data_text[\"text\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spaces :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove useless spaces using regex\n",
    "data_text[\"text\"] = data_text[\"text\"].apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip())\n",
    "\n",
    "# Show 'text' column of the first row\n",
    "print(data_text[\"text\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what we have for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the most common words in the 'text' column of data_text\n",
    "_ = word_occurrences(data_text, visualisation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop-words :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from 'text' column in data_text\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "data_text[\"text\"] = data_text[\"text\"].apply(\n",
    "    lambda x: \" \".join([word for word in x.split() if word not in stop_words])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the most common words in the 'text' column of data_text\n",
    "_ = word_occurrences(data_text, visualisation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data (lemming) :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Define lemmatizer tool\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Show the first row to verify the lemmatization\n",
    "print(\"Before :\\n\\t\", data_text[\"text\"][0])\n",
    "\n",
    "# Apply lemmatization to each word in the text\n",
    "data_text[\"text\"] = data_text[\"text\"].apply(\n",
    "    lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()])\n",
    ")\n",
    "\n",
    "# Show the first row to verify the lemmatization\n",
    "print(\"After :\\n\\t\", data_text[\"text\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the most common words after lemmatization\n",
    "_ = word_occurrences(data_text, visualisation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove outliers :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if there are outliers in occurrences\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(list(word_occurrences(data_text).values()), palette=\"rainbow_r\")\n",
    "plt.xlabel(\"Word Counts\")\n",
    "plt.title(\"Boxplot of Word Counts\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences = word_occurrences(data_text)\n",
    "\n",
    "# Define limits as 10th and 90th percentiles\n",
    "min_occurrences = pd.Series(occurrences).quantile(0.825)\n",
    "max_occurrences = pd.Series(occurrences).quantile(0.955)\n",
    "\n",
    "# momo test\n",
    "min_occurrences = pd.Series(occurrences).quantile(0.825)\n",
    "max_occurrences = pd.Series(occurrences).quantile(0.975)\n",
    "\n",
    "print(\"Min occurrences:\", min_occurrences)\n",
    "print(\"Max occurrences:\", max_occurrences)\n",
    "\n",
    "# Identify words that have more than max_occurrences occurrences\n",
    "high_outliers = {\n",
    "    word: count for word, count in occurrences.items() if count > max_occurrences\n",
    "}\n",
    "\n",
    "# Identify words that have less than min_occurrences occurrences\n",
    "low_outliers = {\n",
    "    word: count for word, count in occurrences.items() if count < min_occurrences\n",
    "}\n",
    "\n",
    "# print(\"High outliers:\", high_outliers)\n",
    "# print(\"Low outliers:\", low_outliers)\n",
    "\n",
    "# Remove words that are in high_outliers and low_outliers from 'text' column in data_text\n",
    "data_text[\"text\"] = data_text[\"text\"].apply(\n",
    "    lambda x: \" \".join(\n",
    "        [\n",
    "            word\n",
    "            for word in x.split()\n",
    "            if word not in high_outliers and word not in low_outliers\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = len(occurrences)\n",
    "after = len(word_occurrences(data_text))\n",
    "difference = before - after\n",
    "\n",
    "print(\"Number of words before removing outliers :\", before)\n",
    "print(\"Number of words after removing outliers  :\", after)\n",
    "print(\"Difference (amount of words removed)     :\", difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if there are outliers in occurrences\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(list(word_occurrences(data_text).values()), palette=\"rainbow_r\")\n",
    "plt.xlabel(\"Word Counts\")\n",
    "plt.title(\"Boxplot of Word Counts\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_word_counts = word_occurrences(data_text, visualisation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data_text variable as a CSV file\n",
    "data_text.to_csv(\"data_text.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Mise en place du moteur de recherche\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous allons maintenant construire la matrice Documents x Termes en adoprant le schéma de pondération TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the vectorizer\n",
    "tf_vectorizer = TfidfVectorizer(use_idf=False)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the vectorizer on the 'text' column\n",
    "X_tf = tf_vectorizer.fit_transform(data_text[\"text\"])\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data_text[\"text\"])\n",
    "\n",
    "# Show the shape of the resulting matrix\n",
    "X_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requete = [\"representation\", \"learning\", \"for\", \"natural\", \"language\", \"processing\"]\n",
    "\n",
    "pseudo_document_tf = tf_vectorizer.transform([\" \".join(requete)])\n",
    "pseudo_document_tfidf = tfidf_vectorizer.transform([\" \".join(requete)])\n",
    "\n",
    "# Calculer les similarités cosinus entre le pseudo-document et les documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "similarities_tf = cosine_similarity(X_tf, pseudo_document_tf)\n",
    "similarities_tfidf = cosine_similarity(X_tfidf, pseudo_document_tfidf)\n",
    "\n",
    "# Afficher les 5 documents les plus similaires\n",
    "top5_tf = np.argsort(similarities_tf.flatten())[::-1][:5]\n",
    "top5_tfidf = np.argsort(similarities_tfidf.flatten())[::-1][:5]\n",
    "\n",
    "print(f\"TF    : {top5_tf} | {similarities_tf[top5_tf].flatten()}\")\n",
    "print(\n",
    "    data_text.iloc[top5_tf].drop(columns=[\"text\", \"abstract\", \"references\"]), \"\\n\\n\\n\\n\"\n",
    ")\n",
    "\n",
    "print(f\"TFIDF : {top5_tfidf} | {similarities_tfidf[top5_tfidf].flatten()}\")\n",
    "print(data_text.iloc[top5_tfidf].drop(columns=[\"text\", \"abstract\", \"references\"]))\n",
    "\n",
    "\n",
    "# print(data_text.iloc[top5_tf][\"title\"], '\\n\\n\\n\\n')\n",
    "# print(data_text.iloc[top5_tfidf][\"title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requete = [\"representation\", \"learning\", \"for\", \"natural\", \"language\", \"processing\"]\n",
    "\n",
    "pseudo_document_tf = tf_vectorizer.transform([\" \".join(requete)])\n",
    "pseudo_document_tfidf = tfidf_vectorizer.transform([\" \".join(requete)])\n",
    "\n",
    "# Calculer les similarités cosinus entre le pseudo-document et les documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "similarities_tf = cosine_similarity(X_tf, pseudo_document_tf)\n",
    "similarities_tfidf = cosine_similarity(X_tfidf, pseudo_document_tfidf)\n",
    "\n",
    "# Afficher les 5 documents les plus similaires\n",
    "top5_tf = np.argsort(similarities_tf.flatten())[::-1][:5]\n",
    "top5_tfidf = np.argsort(similarities_tfidf.flatten())[::-1][:5]\n",
    "\n",
    "print(f\"TF    : {top5_tf} | {similarities_tf[top5_tf].flatten()}\")\n",
    "print(\n",
    "    data_text.iloc[top5_tf].drop(columns=[\"text\", \"abstract\", \"references\"]), \"\\n\\n\\n\\n\"\n",
    ")\n",
    "\n",
    "print(f\"TFIDF : {top5_tfidf} | {similarities_tfidf[top5_tfidf].flatten()}\")\n",
    "print(data_text.iloc[top5_tfidf].drop(columns=[\"text\", \"abstract\", \"references\"]))\n",
    "\n",
    "\n",
    "# print(data_text.iloc[top5_tf][\"title\"], '\\n\\n\\n\\n')\n",
    "# print(data_text.iloc[top5_tfidf][\"title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ecrire meme code que cellule avant just change le score de similarité a distance euclidienne\n",
    "similarities_tf = euclidean_distances(X_tf, pseudo_document_tf)\n",
    "similarities_tfidf = euclidean_distances(X_tfidf, pseudo_document_tfidf)\n",
    "\n",
    "# Afficher les 5 documents les plus similaires\n",
    "top5_tf = np.argsort(similarities_tf.flatten())[::-1][:5]\n",
    "top5_tfidf = np.argsort(similarities_tfidf.flatten())[::-1][:5]\n",
    "\n",
    "print(f\"TF    : {top5_tf} | {similarities_tf[top5_tf].flatten()}\")\n",
    "print(\n",
    "    data_text.iloc[top5_tf].drop(columns=[\"text\", \"abstract\", \"references\"]), \"\\n\\n\\n\\n\"\n",
    ")\n",
    "\n",
    "print(f\"TFIDF : {top5_tfidf} | {similarities_tfidf[top5_tfidf].flatten()}\")\n",
    "print(data_text.iloc[top5_tfidf].drop(columns=[\"text\", \"abstract\", \"references\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk about :\n",
    "\n",
    "- TF requiring stopwords removed, and TDIDF requiring stop words not removed\n",
    "- Euclidian distances donc function in higher dimensions (if you do TF and TFIDF scores with euclidian distances, you'll get the exact same score every time (curse of high dimensionnality))\n",
    "- Don't remove stopwords ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save X_tf and X_tfidf as numpy matrix\n",
    "np.save(\"X_tf.npy\", X_tf)\n",
    "np.save(\"X_tfidf.npy\", X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_netanalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
